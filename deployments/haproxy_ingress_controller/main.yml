---
# namespace for å sortere ressurser
apiVersion: v1
kind: Namespace
metadata:
  name: haproxy
---
# for å sikkre at det er alltid er 1 pod med haproxy binary up and running.
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/name: haproxy-ingress
  name: haproxy-ingress
  namespace: haproxy
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: haproxy-ingress
  minAvailable: 1
---
# en service account slik at haproxy pod'en skal ha tillgang til kubernetes API
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/name: haproxy-ingress
  name: haproxy-ingress
  namespace: haproxy
---
# en config map som er hovedkonfigurasjonen for haproxy. cluster-wide config
# healthz-port: Define the port number HAProxy should listen to in order to answer for health checking requests. Use /healthz as the request path.
# stats-port: er basicly porten for HAProxy statestikk siden.
# https://haproxy-ingress.github.io/docs/configuration/keys/
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/name: haproxy-ingress
  name: haproxy-ingress
  namespace: haproxy
data:
  healthz-port: "10253"
  stats-port: "1936"
---
# definerer tillganger til kubernetes API'et
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: haproxy-ingress
  name: haproxy-ingress
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - endpoints
      - nodes
      - secrets
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - namespaces
      - nodes
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - pods
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
      - networking.k8s.io
    resources:
      - ingresses
      - ingressclasses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - gateway.networking.k8s.io
      - networking.x-k8s.io
    resources:
      - gateways
      - gatewayclasses
      - httproutes
      - tlsroutes
      - tcproutes
      - udproutes
      - backendpolicies
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - extensions
      - networking.k8s.io
    resources:
      - ingresses/status
    verbs:
      - update
---
# binder service account "haproxy-ingress" til ClusterRole "haproxy-ingress"
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/name: haproxy-ingress
  name: haproxy-ingress
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: haproxy-ingress
subjects:
  - kind: ServiceAccount
    name: haproxy-ingress
    namespace: haproxy
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: haproxy-ingress
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/name: haproxy-ingress
  name: haproxy-ingress
  namespace: haproxy
rules:
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - get
      - create
      - update
  - apiGroups:
      - ""
    resources:
      - pods
      - secrets
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
      - endpoints
    verbs:
      - get
      - create
      - update
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/name: haproxy-ingress
  name: haproxy-ingress
  namespace: haproxy
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: haproxy-ingress
subjects:
  - kind: ServiceAccount
    name: haproxy-ingress
    namespace: haproxy
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: haproxy-ingress
---
# LoadBalancer: Exposes the Service externally using a cloud provider's load balancer.
# By setting .spec.externalTrafficPolicy to Local, the client IP addresses is propagated to the end Pods,
# but this could result in uneven distribution of traffic.
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/name: haproxy-ingress
  name: haproxy-ingress
  namespace: haproxy
spec:
  externalTrafficPolicy: "Local"
  ports:
    - name: "http-80"
      port: 80
      protocol: TCP
      targetPort: http
    - name: "https-443"
      port: 443
      protocol: TCP
      targetPort: https
  selector:
    app.kubernetes.io/name: haproxy-ingress
  type: "LoadBalancer"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: haproxy-ingress
  name: haproxy-ingress
  namespace: haproxy
spec:
  replicas: 1
# used to specify how many old ReplicaSets for this Deployment you want to retain.
# The rest will be garbage-collected in the background. By default, it is 10.
  revisionHistoryLimit:
  strategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
# default = 0, hvor mange sekunder kubernetes skal vente etter podden ikke "failer"
  minReadySeconds: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: haproxy-ingress
  template:
    metadata:
      annotations:
      labels:
        app.kubernetes.io/name: haproxy-ingress
    spec:
# tilgang til kubernetes api
      serviceAccountName: haproxy-ingress
      containers:
        - name: haproxy-ingress
          image: "quay.io/jcmoraisjr/haproxy-ingress:v0.14.2"
          imagePullPolicy: "IfNotPresent"
          args:
            - --configmap=haproxy/haproxy-ingress
            - --ingress-class=haproxy
            - --sort-backends
          ports:        
            - name: http
              containerPort: 80
            - name: https
              containerPort: 443
            - name: healthz
              containerPort: 10253      
          livenessProbe:
            httpGet:
              path: "/healthz"
              port: 10253
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: "/healthz"
              port: 10253
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 3
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          resources: {}
      terminationGracePeriodSeconds: 60
      dnsPolicy: ClusterFirst
      hostNetwork: false